{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce486179",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"monai.png\"/></center>\n",
    "\n",
    "<p style=\"text-align: center\">Welcome to the MONAI bootcamp!</p>\n",
    "<p style=\"text-align: center\">This notebook will introduce you to an end-to-end working in MONAI using a standard PyTorch loop.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11ee02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Google Colab\n",
    "\n",
    "This notebook has the pip command for installing MONAI and will be added to any subsequent notebook.\n",
    "\n",
    "### Enabling GPU Support\n",
    "\n",
    "To use GPU resources through Colab, change the runtime to GPU:\n",
    "\n",
    "1. From the **\"Runtime\"** menu select **\"Change Runtime Type\"**\n",
    "2. Choose **\"GPU\"** from the drop-down menu\n",
    "3. Click **\"SAVE\"**\n",
    "\n",
    "This will reset the notebook and probably ask you if you are a robot (these instructions assume you are not). Running\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a61564",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Verify GPU Access\n",
    "\n",
    "Running **!nvidia-smi** in a cell will verify this has worked and show you what kind of hardware you have access to.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed430fcc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3c4cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Required Packages for Colab Execution\n",
    "\n",
    "Execute the following cell to install MONAI the first time a colab notebook is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba13cca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==1.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b3613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## End-to-end Training with Pytorch\n",
    "\n",
    "We've covered a lot of material and now it's time to apply the things that we've learned in an end-to-end example using the basic Pytorch paradigm. We'll cover:\n",
    "\n",
    "1. **Setting up our Dataset and exploring the data**\n",
    "2. **Preparing datasets and transforms**\n",
    "3. **Define your network and create our PyTorch training loop**\n",
    "4. **Evaluate your model and understand the results**\n",
    "5. **Later: use Ignite-based workflow classes to simplify process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226046f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Later: Bundles and Model Zoo\n",
    "\n",
    "* MONAI Bundle format is a self-contained model package with pre-trained weights and all associated metadata abstracted through JSON and YAML-based configurations. By focusing on ease of use and flexibility, you can directly override or customize these configs or utilize a hybrid programming model that supports config to Python Code abstraction.\n",
    "\n",
    "* Model Zoo provides pre-trained models as bundles. With these two components we hope to establish a common standard for reproducible research and collaboration. Everyone is welcome to contribute to this effort by submitting their pre-trained models for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99dede4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imports\n",
    "\n",
    "Let's get started by importing our dependecies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a1039",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import monai\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.data import decollate_batch, partition_dataset_classes\n",
    "from monai.networks.nets import DenseNet121\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst, Compose,LoadImage,RandFlip, RandRotate,\n",
    "    RandZoom, ScaleIntensity, Activations, AsDiscrete, EnsureType\n",
    ")\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19564115",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Setting up our Dataset and Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6cf03",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Setup data directory\n",
    "\n",
    "We'll create a temporary directory for all the MONAI data we're going to be using called MONAI_DATA_DIRECTORY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc4d02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4a4a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Download the MedNIST dataset\n",
    "The `MedNIST` dataset was gathered from several sets from [TCIA](https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions),\n",
    "[the RSNA Bone Age Challenge](http://rsnachallenges.cloudapp.net/competitions/4),\n",
    "and [the NIH Chest X-ray dataset](https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest).\n",
    "\n",
    "The dataset is kindly made available by [Dr. Bradley J. Erickson M.D., Ph.D.](https://www.mayo.edu/research/labs/radiology-informatics/overview) (Department of Radiology, Mayo Clinic)\n",
    "under the Creative Commons [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/). If you use the MedNIST dataset, please acknowledge the source.\n",
    "\n",
    "Download this dataset and extract it into the temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e62a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "resource = \"https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz?dl=1\"\n",
    "md5 = \"0bc7306e7427e00ad1c5526a6677552d\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"MedNIST.tar.gz\")\n",
    "data_dir = os.path.join(root_dir, \"MedNIST\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f60c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Set deterministic training for reproducibility\n",
    "\n",
    "[set_determinism](https://docs.monai.io/en/latest/utils.html?highlight=set_determinism#monai.utils.misc.set_determinism) will set the random seeds in both Numpy and PyTorch to ensure reproducibility. We'll see later that we need to go a little bit further to ensure reproducibility in a jupyter notebook.  For now, we'll also instanitate a seed value that we can use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981d397",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "rseed = 12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f4544",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Read the image filenames from the dataset folders\n",
    "\n",
    "When using a dataset, you want to understand the basics of the images, labels, and more.  We'll start off by showing some of those basic statistics for MedNIST. \n",
    "\n",
    "We'll then define our own `MedNISTDataset` class for illustrative purposes, although a more robust version is provided in MONAI.\n",
    "\n",
    "MedNIST has 6 different folders representing 6 different categories: Hand, AbdomenCT, CXR, ChestCT, BreastMRI, HeadCT.  We'll be using each of these categories as our label names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65125ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "subdirs = sorted(filter(os.path.isdir, glob(f\"{data_dir}/*\")))\n",
    "class_names = list(map(os.path.basename, subdirs))\n",
    "num_class = len(class_names)\n",
    "\n",
    "image_files = [sorted(glob(f\"{d}/*\")) for d in subdirs]\n",
    "\n",
    "num_each = [len(image_files[i]) for i in range(num_class)]\n",
    "image_files_list = sum(image_files, [])\n",
    "image_class = sum([[i] * n for i, n in enumerate(num_each)], [])\n",
    "\n",
    "num_total = len(image_class)\n",
    "image_width, image_height = PIL.Image.open(image_files_list[0]).size\n",
    "\n",
    "print(f\"Total image count: {num_total}\")\n",
    "print(f\"Image dimensions: {image_width} x {image_height}\")\n",
    "print(f\"Label names: {class_names}\")\n",
    "print(f\"Label counts: {num_each}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a405db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Randomly pick images from the dataset to visualize and check\n",
    "\n",
    "We want to understand what the images we're using look like, so we'll start by visualizing a few random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4164108",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(2, 5, figsize=(8, 4))\n",
    "for i, k in enumerate(np.random.randint(num_total, size=10)):\n",
    "    im = PIL.Image.open(image_files_list[k])\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(class_names[image_class[k]])\n",
    "    plt.imshow(np.array(im), cmap=\"gray\", vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa5271",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Preparing datasets and transforms\n",
    "\n",
    "We want to split the data into 3 different sets, one for training, one for validation, and one for testing.  We'll use a ratio of 80/10/10 for those sets.\n",
    "\n",
    "[MONAI Documentation](https://docs.monai.io/en/stable/)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Activity</b>\n",
    "\n",
    "- Utilize the partition_dataset_classes function to properly split the dataset into parts\n",
    "- Make sure you use the rseed variable from above in your partition_dataset_classes function to ensure reproducability\n",
    "- We'll use an 80/10/10 split for the dataset\n",
    "- Use the parts to create the new image lists and label lists\n",
    "- Create new train, val, and test subsets, eaching having an image list (ie. `train_x`) and label list (`train_y`)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142f52d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "parts = partition_dataset_classes(???)\n",
    "    \n",
    "???\n",
    "\n",
    "print(f\"Training count: {len(train_x)}, Validation count: {len(val_x)}, Test count: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532d7a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Define MONAI transforms, Dataset and Dataloader to pre-process data\n",
    "\n",
    "We'll define our transform sequence with `Compose`, in which we'll load the image, add a channel, scale its intensity, and utilize a few random functions.\n",
    "\n",
    "[MONAI Documentation](https://docs.monai.io/en/stable/)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Activity</b>\n",
    "    \n",
    "Below you'll define the Compose trainsform train for the training data.  You'll need the following transforms:\n",
    "    \n",
    "- Load Image\n",
    "- Ensure Channel First\n",
    "- Scale Intensity\n",
    "- Random Rotate, Flip, and Zoom\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314e5b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rseed = 12345678\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [ ??? ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [ ??? ]\n",
    ")\n",
    "\n",
    "act = Compose([Activations(softmax=True)])\n",
    "to_onehot = Compose([AsDiscrete(to_onehot=num_class)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76c429-df17-4709-80d4-c46e88d694b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform Test Functionality\n",
    "plt.subplots(1, 3, figsize=(4, 4))\n",
    "for i in range(0,3):\n",
    "    test_output = train_transforms(image_files_list[i])\n",
    "    #test_output = val_transforms(image_files_list[i])    \n",
    "    \n",
    "    arr = np.array(test_output[0])\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.xlabel(class_names[image_class[k]])\n",
    "    plt.imshow(arr, cmap=\"gray\", vmin=0, vmax=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5951933",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Initialise the datasets and loaders for training, validation and test sets\n",
    " * Define a simple dataset, that we'll call `MedNISTDataset`, that groups:\n",
    "   * Images\n",
    "   * Labels\n",
    "   * The transforms that are to be run on the images and labels\n",
    " * Create three instances of this dataset:\n",
    "   * One for training\n",
    "   * One for validation\n",
    "   * One for testing\n",
    "   \n",
    "We'll use a batch size of 512 and employ 10 workers to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d34ddf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 10\n",
    "\n",
    "class MedNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_files, labels, transforms):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transforms(self.image_files[index]), self.labels[index]\n",
    "\n",
    "\n",
    "train_ds = MedNISTDataset(train_x, train_y, train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_ds = MedNISTDataset(val_x, val_y, val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "test_ds = MedNISTDataset(test_x, test_y, val_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea161d42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Define network and optimizer\n",
    "\n",
    "We'll make sure to set the relevant values for getting our device and instantiating our network, loss function and optimizer.\n",
    "\n",
    "[MONAI Documentation](https://docs.monai.io/en/stable/)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Activity</b>\n",
    "    \n",
    "Do the following:\n",
    "1. Get the device for a GPU from torch\n",
    "2. Instantiate DenseNet121.  You'll need to set Spatial Dimension to 2 and Input Channels to 1 and output channels to the number of classes from our data exploration above.\n",
    "3. Instantiate the Torch CrossEntropy Loss function\n",
    "4. Instantiate the Torch  Adam Optimizer with a learning rate of 1e-5  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cc896",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "device = ???\n",
    "net = ???\n",
    "loss_function = ???\n",
    "optimizer = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88527bd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Network training\n",
    "We are hand-rolling a basic pytorch training loop here:\n",
    " * standard pytorch training loop\n",
    "   * step through each training epoch, running through the training set in batches\n",
    "   * after each epoch, run a validation pass, evaluating the network\n",
    "   * if it shows improved performance, save out the model weights\n",
    " * later we will revisit training loops in a more Ignite / MONAI fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe3627",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "epoch_num = 4\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "metric_values = list()\n",
    "auc_metric = ROCAUCMetric()\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "\n",
    "    epoch_loss = 0\n",
    "    step = 1\n",
    "\n",
    "    steps_per_epoch = len(train_ds) // train_loader.batch_size\n",
    "\n",
    "    # put the network in train mode; this tells the network and its modules to\n",
    "    # enable training elements such as normalisation and dropout, where applicable\n",
    "    net.train()\n",
    "    for batch_data in train_loader:\n",
    "\n",
    "        # move the data to the GPU\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "\n",
    "        # prepare the gradients for this step's back propagation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # run the network forwards\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # run the loss function on the outputs\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # tell the optimizer to update the weights according to the gradients\n",
    "        # and its internal optimisation strategy\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size + 1}, training_loss: {loss.item():.4f}\")\n",
    "        step += 1\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # after each epoch, run our metrics to evaluate it, and, if they are an improvement,\n",
    "    # save the model out\n",
    "    \n",
    "    # switch off training features of the network for this pass\n",
    "    net.eval()\n",
    "\n",
    "    # 'with torch.no_grad()' switches off gradient calculation for the scope of its context\n",
    "    with torch.no_grad():\n",
    "        # create lists to which we will concatenate the the validation results\n",
    "        preds = list()\n",
    "        labels = list()\n",
    "\n",
    "        # iterate over each batch of images and run them through the network in evaluation mode\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "\n",
    "            # run the network\n",
    "            val_pred = net(val_images)\n",
    "\n",
    "            preds.append(val_pred)\n",
    "            labels.append(val_labels)\n",
    "\n",
    "        # concatenate the predicted labels with each other and the actual labels with each other\n",
    "        y_pred = torch.cat(preds)\n",
    "        y = torch.cat(labels)\n",
    "\n",
    "        # we are using the area under the receiver operating characteristic (ROC) curve to determine\n",
    "        # whether this epoch has improved the best performance of the network so far, in which case\n",
    "        # we save the network in this state\n",
    "        y_onehot = [to_onehot(i) for i in decollate_batch(y, detach=False)]        \n",
    "        y_pred_act = [act(i) for i in decollate_batch(y_pred)]\n",
    "        \n",
    "        auc_metric(y_pred_act, y_onehot)\n",
    "        auc_value = auc_metric.aggregate()\n",
    "        auc_metric.reset()\n",
    "        metric_values.append(auc_value)\n",
    "        \n",
    "        acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "        acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "        \n",
    "        if auc_value > best_metric:\n",
    "            best_metric = auc_value\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(net.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
    "            print(\"saved new best metric network\")\n",
    "            \n",
    "        print(\n",
    "            f\"current epoch: {epoch + 1} current AUC: {auc_value:.4f} /\"\n",
    "            f\" current accuracy: {acc_metric:.4f} best AUC: {best_metric:.4f} /\"\n",
    "            f\" at epoch: {best_metric_epoch}\"\n",
    "        )\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a48ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Plot the loss and metric\n",
    "\n",
    "Once we're done training we want to visualize our Loss and Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816b762",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [(i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fdeec9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **4. Evaluate Your Model and Understand the Results**\n",
    "\n",
    "After training and validation, we now have the best model as determined by the validation dataset.  But now we need to evaluate the model on the test dataset to check whether the final model is robust and not over-fitting.  We'll use these predictions to generate a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecec300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "net.eval()\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data[0].to(device),\n",
    "            test_data[1].to(device),\n",
    "        )\n",
    "        pred = net(test_images).argmax(dim=1)\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6d363",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Some light analytics - classification report\n",
    "\n",
    "We'll utilize scikit-learn's classification report to get the precision, recall, and f1-score for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60255c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11426cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Some light analytics - confusion matrix\n",
    "\n",
    "Let's also create a confusion matrix to get a better understanding of the failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024bb27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cmat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cax = plt.matshow(cmat, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "plt.colorbar(cax)\n",
    "\n",
    "cax.axes.set_xticks(list(range(len(class_names))), class_names, rotation=270)\n",
    "cax.axes.set_yticks(list(range(len(class_names))), class_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab4c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Summary**\n",
    "\n",
    "In this notebook, we went through an end-to-end workflow to train the MedNIST dataset using a densenet121 network.  Along the way, you did the following:\n",
    "- Learned about the MedNIST Data and downloaded it\n",
    "- Visualized the data to understand the images\n",
    "- Setup the datasets for use in the model training\n",
    "- Defined our transforms, datasets, network, and optimizers\n",
    "- Trained a densenet model and saved the best model as determined by the validation accuracy\n",
    "- Plotted your training results\n",
    "- Evaluated your model against the test set\n",
    "- Ran your final predictions through a classification report to understand more about your final results\n",
    "- Created a new workflow using Ignite\n",
    "- Learn more about issues with determinism and how to look out for pitfalls"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:monai]",
   "language": "python",
   "name": "conda-env-monai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
