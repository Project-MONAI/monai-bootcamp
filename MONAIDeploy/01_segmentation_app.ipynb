{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MONAI Logo](monai.png)\n",
    "\n",
    "# MONAI Deploy App SDK - Segmentation Application\n",
    "\n",
    "This tutorial shows how to create an organ segmentation application for a PyTorch model that has been trained with MONAI, and visualize the segmentation and input images with Clara Viz integration.\n",
    "\n",
    "First, you will need to design the workflow of your application that defines Operators (tasks) and flows among them. Once the workflow is designed, you can use existing operators provided by MONAI Deploy or start implementing your customer operator classes. Next, you'll implement an Application class to construct a workflow graph with the operators.\n",
    "\n",
    "![MONAI Deploy App SDK Workflow](images/sdk_workflow.png)\n",
    "\n",
    "You can execute and debug your application locally in a Jupyter notebook or CLI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Deploying AI models requires the integration with clinical imaging network, even if in a for-research-use setting. This means that the AI deploy application will need to support standards-based imaging protocols, and specifically for Radiological imaging, DICOM protocol.\n",
    "\n",
    "Typically, DICOM network communication, either in DICOM TCP/IP network protocol or DICOMWeb, would be handled by DICOM devices or services, e.g. MONAI Deploy Informatics Gateway, so the deploy application itself would only need to use DICOM Part 10 files as input and save the AI result in DICOM Part10 file(s). For segmentation use cases, the DICOM instance file could be a DICOM Segmentation object or a DICOM RT Structure Set, and for classification, DICOM Structure Report and/or DICOM Encapsulated PDF.\n",
    "\n",
    "During model training, input and label images are typically in non-DICOM volumetric image format, e.g., NIfTI and PNG, converted from a specific DICOM study series. Furthermore, the voxel spacings most likely have been re-sampled to be uniform for all images. When integrated with imaging networks and receiving DICOM instances from modalities and Picture Archiving and Communications System, PACS, an AI deploy application may have to deal with a whole DICOM study with multiple series, whose images' spacing may not be the same as expected by the trained model. To address these cases consistently and efficiently, MONAI Deploy Application SDK provides classes, called operators, to parse DICOM studies, select specific series with application-defined rules, and convert the selected DICOM series into domain-specific image format along with meta-data representing the pertinent DICOM attributes.\n",
    "\n",
    "---\n",
    " > **Note**: For local testing, if there is a lack of DICOM Part 10 files, one can use open source programs, e.g., 3D Slicer, to convert NIfTI to DICOM files.\n",
    "---\n",
    "\n",
    "In the following sections, we will demonstrate how to create a MONAI Deploy application package using the MONAI Deploy App SDK.\n",
    "\n",
    "### Table of Contents\n",
    "* [Setup](#1.-Setup)\n",
    "* [Operators](#2.-Operators)\n",
    "* [Python Files](#3.-Python-Files)\n",
    "* [Packaging App](#4-Packaging-app-(creating-MAP-Docker-image))\n",
    "* [Conclusion](#5.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Google Colab\n",
    "\n",
    "This notebook has the pip command for installing MONAI and will be added to any subsequent notebook.\n",
    "\n",
    "**Enabling GPU Support**\n",
    "\n",
    "To use GPU resources through Colab, change the runtime to GPU:\n",
    "\n",
    "1. From the **\"Runtime\"** menu select **\"Change Runtime Type\"**\n",
    "2. Choose **\"GPU\"** from the drop-down menu\n",
    "3. Click **\"SAVE\"**\n",
    "\n",
    "This will reset the notebook and probably ask you if you are a robot (these instructions assume you are not)\n",
    "\n",
    "### Verify GPU Access\n",
    "\n",
    "Running **!nvidia-smi** in a cell will verify this has worked and show you what kind of hardware you have access to.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary image loading/processing packages for the application\n",
    "!pip install -q \"Pillow\"\n",
    "!pip install -q \"scikit-image\"\n",
    "!pip install -q \"wget\"\n",
    "!pip install -q \"pydicom\"\n",
    "!pip install -q \"highdicom\"\n",
    "!pip install -q \"matplotlib\"\n",
    "!pip install -q \"typeguard==2.12.1\"\n",
    "%matplotlib inline\n",
    "\n",
    "# Install MONAI Deploy App SDK package\n",
    "!pip install -qU \"monai-deploy-app-sdk\"\n",
    "!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==1.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "To begin, check that the NVIDIA driver has been installed correctly. The `nvidia-smi` command should run and output information about the GPUs on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup environment\n",
    "\n",
    "We'll set up folder called notebook_1 where we'll extract our data, models, and write out output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NOTEBOOK_ROOT=\"notebook_1/\"\n",
    "!mkdir -p notebook_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setup imports\n",
    "\n",
    "Let's import necessary classes/decorators to define Application and Operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os import path\n",
    "from skimage import filters, io\n",
    "from numpy import uint8\n",
    "\n",
    "import monai.deploy.core as md\n",
    "from monai.deploy.core import ExecutionContext, Image, InputContext, IOType, Operator, OutputContext\n",
    "from monai.deploy.operators.monai_seg_inference_operator import InMemImageReader, MonaiSegInferenceOperator\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    ")\n",
    "\n",
    "# Required for setting SegmentDescription attributes. Direct import as this is not part of App SDK package.\n",
    "from pydicom.sr.codedict import codes\n",
    "\n",
    "from monai.deploy.core import Application, resource\n",
    "from monai.deploy.operators.dicom_data_loader_operator import DICOMDataLoaderOperator\n",
    "from monai.deploy.operators.dicom_seg_writer_operator import DICOMSegmentationWriterOperator, SegmentDescription\n",
    "from monai.deploy.operators.dicom_series_selector_operator import DICOMSeriesSelectorOperator\n",
    "from monai.deploy.operators.dicom_series_to_volume_operator import DICOMSeriesToVolumeOperator\n",
    "from monai.deploy.operators.clara_viz_operator import ClaraVizOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data Setup\n",
    "\n",
    "Download/Extract ai_spleen_seg_data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ai_spleen_bundle_data test data zip file\n",
    "!gdown \"https://drive.google.com/uc?id=1Uds8mEvdGNYUuvFpTtCQ8gNU97bAPCaQ\"\n",
    "\n",
    "# After downloading ai_spleen_bundle_data zip file from the web browser or using gdown,\n",
    "!unzip -o \"ai_spleen_seg_bundle_data.zip\" -d \"notebook_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operators\n",
    "\n",
    "### 2.1 Creating Model Specific Inference Operator classes\n",
    "\n",
    "Each Operator class inherits [Operator](/modules/_autosummary/monai.deploy.core.Operator) class and input/output properties are specified by using [@input](/modules/_autosummary/monai.deploy.core.input)/[@output](/modules/_autosummary/monai.deploy.core.output) decorators.\n",
    "\n",
    "Business logic would be implemented in the <a href=\"../../modules/_autosummary/monai.deploy.core.Operator.html#monai.deploy.core.Operator.compute\">compute()</a> method.\n",
    "\n",
    "The App SDK provides a `MonaiSegInferenceOperator` class to perform segmentation prediction with a Torch Script model. For consistency, this class uses MONAI dictionary-based transforms, as `Compose` object, for pre and post transforms. The model-specific inference operator will then only need to create the pre and post transform `Compose` based on what has been used in the model training and validation. Note that for deploy application, `ignite` is not needed nor supported.\n",
    "\n",
    "The workflow of the application would look like this.\n",
    "\n",
    "![DICOM Operator Workflow](images/dicom_workflow.png)\n",
    "\n",
    "\n",
    "### 2.2 Spleen Operator\n",
    "\n",
    "\n",
    "The `SpleenSegOperator` gets as input an in-memory [Image](/modules/_autosummary/monai.deploy.core.domain.Image) object that has been converted from a DICOM CT series by the preceding `DICOMSeriesToVolumeOperator`, and as output in-memory segmentation [Image](/modules/_autosummary/monai.deploy.core.domain.Image) object.\n",
    "\n",
    "The `pre_process` function creates the pre-transforms `Compose` object. For `LoadImage`, a specialized `InMemImageReader`, derived from MONAI `ImageReader`, is used to convert the in-memory pixel data and return the `numpy` array as well as the meta-data. Also, the DICOM input pixel spacings are often not the same as expected by the model, so the `Spacingd` transform must be used to re-sample the image with the expected spacing.\n",
    "\n",
    "The `post_process` function creates the post-transform `Compose` object. The `SaveImageD` transform class is used to save the segmentation mask as NIfTI image file, which is optional as the in-memory mask image will be passed down to the DICOM Segmentation writer for creating a DICOM Segmentation instance. The `Invertd` must also be used to revert the segmentation image's orientation and spacing to be the same as the input.\n",
    "\n",
    "When the `MonaiSegInferenceOperator` object is created, the `ROI` size is specified, as well as the transform `Compose` objects. Furthermore, the dataset image key names are set accordingly.\n",
    "\n",
    "Loading of the model and performing the prediction are encapsulated in the `MonaiSegInferenceOperator` and other SDK classes. Once the inference is completed, the segmentation [Image](/modules/_autosummary/monai.deploy.core.domain.Image) object is created and set to the output (<a href=\"../../modules/_autosummary/monai.deploy.core.OutputContext.html#monai.deploy.core.OutputContext.set\">op_output.set(value, label)</a>), by the `MonaiSegInferenceOperator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@md.input(\"image\", Image, IOType.IN_MEMORY)\n",
    "@md.output(\"seg_image\", Image, IOType.IN_MEMORY)\n",
    "@md.env(pip_packages=[\"monai>=0.8.1\", \"torch>=1.5\", \"numpy>=1.21\", \"nibabel\"])\n",
    "class SpleenSegOperator(Operator):\n",
    "    \"\"\"Performs Spleen segmentation with a 3D image converted from a DICOM CT series.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.logger = logging.getLogger(\"{}.{}\".format(__name__, type(self).__name__))\n",
    "        super().__init__()\n",
    "        self._input_dataset_key = \"image\"\n",
    "        self._pred_dataset_key = \"pred\"\n",
    "\n",
    "    def compute(self, op_input: InputContext, op_output: OutputContext, context: ExecutionContext):\n",
    "\n",
    "        input_image = op_input.get(\"image\")\n",
    "        if not input_image:\n",
    "            raise ValueError(\"Input image is not found.\")\n",
    "\n",
    "        output_path = context.output.get().path\n",
    "\n",
    "        # This operator gets an in-memory Image object, so a specialized ImageReader is needed.\n",
    "        _reader = InMemImageReader(input_image)\n",
    "        pre_transforms = self.pre_process(_reader)\n",
    "        post_transforms = self.post_process(pre_transforms, path.join(output_path, \"prediction_output\"))\n",
    "\n",
    "        # Delegates inference and saving output to the built-in operator.\n",
    "        infer_operator = MonaiSegInferenceOperator(\n",
    "            (\n",
    "                96,\n",
    "                96,\n",
    "                96,\n",
    "            ),\n",
    "            pre_transforms,\n",
    "            post_transforms,\n",
    "        )\n",
    "\n",
    "        # Setting the keys used in the dictironary based transforms may change.\n",
    "        infer_operator.input_dataset_key = self._input_dataset_key\n",
    "        infer_operator.pred_dataset_key = self._pred_dataset_key\n",
    "\n",
    "        # Now let the built-in operator handles the work with the I/O spec and execution context.\n",
    "        infer_operator.compute(op_input, op_output, context)\n",
    "\n",
    "    def pre_process(self, img_reader) -> Compose:\n",
    "        \"\"\"Composes transforms for preprocessing input before predicting on a model.\"\"\"\n",
    "\n",
    "        my_key = self._input_dataset_key\n",
    "        return Compose(\n",
    "            [\n",
    "                LoadImaged(keys=my_key, reader=img_reader),\n",
    "                EnsureChannelFirstd(keys=my_key),\n",
    "                Orientationd(keys=my_key, axcodes=\"RAS\"),\n",
    "                Spacingd(keys=my_key, pixdim=[1.5, 1.5, 2.9], mode=[\"bilinear\"]),\n",
    "                ScaleIntensityRanged(keys=my_key, a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "                EnsureTyped(keys=my_key),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def post_process(self, pre_transforms: Compose, out_dir: str = \"./prediction_output\") -> Compose:\n",
    "        \"\"\"Composes transforms for postprocessing the prediction results.\"\"\"\n",
    "\n",
    "        pred_key = self._pred_dataset_key\n",
    "        return Compose(\n",
    "            [\n",
    "                Activationsd(keys=pred_key, softmax=True),\n",
    "                Invertd(\n",
    "                    keys=pred_key,\n",
    "                    transform=pre_transforms,\n",
    "                    orig_keys=self._input_dataset_key,\n",
    "                    nearest_interp=False,\n",
    "                    to_tensor=True,\n",
    "                ),\n",
    "                AsDiscreted(keys=pred_key, argmax=True),\n",
    "                SaveImaged(\n",
    "                    keys=pred_key,\n",
    "                    output_dir=out_dir,\n",
    "                    output_postfix=\"seg\",\n",
    "                    output_dtype=uint8,\n",
    "                ),\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Creating Application class\n",
    "\n",
    "Our application class would look like below.\n",
    "\n",
    "It defines `App` class, inheriting [Application](/modules/_autosummary/monai.deploy.core.Application) class.\n",
    "\n",
    "The requirements (resource and package dependency) for the App can be specified by using [@resource](/modules/_autosummary/monai.deploy.core.resource) and [@env](/modules/_autosummary/monai.deploy.core.env) decorators.\n",
    "\n",
    "The base class method, `compose`, is overridden. Objects required for DICOM parsing, series selection (selecting the first series for the current release), pixel data conversion to volume image, and segmentation instance creation are created, so is the model-specific `SpleenSegOperator`. The execution pipeline, as a Directed Acyclic Graph, is created by connecting these objects through <a href=\"../../modules/_autosummary/monai.deploy.core.Application.html#monai.deploy.core.Application.add_flow\">self.add_flow()</a>.\n",
    "\n",
    "We have multiple powerful operators in this application:\n",
    "1. DICOMDataLoaderOperator\n",
    "2. DICOMSeriesSelectorOperator\n",
    "3. DICOMSeriesToVolumeOperator\n",
    "4. SpleenSegOperator\n",
    "5. DICOMSegmentationWriterOperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@resource(cpu=1, gpu=1, memory=\"7Gi\")\n",
    "class AISpleenSegApp(Application):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Creates an application instance.\"\"\"\n",
    "\n",
    "        self._logger = logging.getLogger(\"{}.{}\".format(__name__, type(self).__name__))\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def run(self, *args, **kwargs):\n",
    "        # This method calls the base class to run. Can be omitted if simply calling through.\n",
    "        self._logger.debug(f\"Begin {self.run.__name__}\")\n",
    "        super().run(*args, **kwargs)\n",
    "        self._logger.debug(f\"End {self.run.__name__}\")\n",
    "\n",
    "    def compose(self):\n",
    "        \"\"\"Creates the app specific operators and chain them up in the processing DAG.\"\"\"\n",
    "\n",
    "        self._logger.debug(f\"Begin {self.compose.__name__}\")\n",
    "        # Creates the custom operator(s) as well as SDK built-in operator(s).\n",
    "        study_loader_op = DICOMDataLoaderOperator()\n",
    "        series_selector_op = DICOMSeriesSelectorOperator(rules=Sample_Rules_Text)\n",
    "        series_to_vol_op = DICOMSeriesToVolumeOperator()\n",
    "        # Model specific inference operator, supporting MONAI transforms.\n",
    "\n",
    "        # Creates the model specific segmentation operator\n",
    "        spleen_seg_op = SpleenSegOperator()\n",
    "\n",
    "        # Create DICOM Seg writer providing the required segment description for each segment with\n",
    "        # the actual algorithm and the pertinent organ/tissue.\n",
    "        # The segment_label, algorithm_name, and algorithm_version are limited to 64 chars.\n",
    "        # https://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_6.2.html\n",
    "        # User can Look up SNOMED CT codes at, e.g.\n",
    "        # https://bioportal.bioontology.org/ontologies/SNOMEDCT\n",
    "\n",
    "        _algorithm_name = \"3D segmentation of the Spleen from a CT series\"\n",
    "        _algorithm_family = codes.DCM.ArtificialIntelligence\n",
    "        _algorithm_version = \"0.1.0\"\n",
    "\n",
    "        segment_descriptions = [\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Lung\",\n",
    "                segmented_property_category=codes.SCT.Organ,\n",
    "                segmented_property_type=codes.SCT.Lung,\n",
    "                algorithm_name=_algorithm_name,\n",
    "                algorithm_family=_algorithm_family,\n",
    "                algorithm_version=_algorithm_version,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        custom_tags = {\"SeriesDescription\": \"AI generated Seg, not for clinical use.\"}\n",
    "\n",
    "        dicom_seg_writer = DICOMSegmentationWriterOperator(\n",
    "            segment_descriptions=segment_descriptions, custom_tags=custom_tags\n",
    "        )\n",
    "\n",
    "        # Create the processing pipeline, by specifying the source and destination operators, and\n",
    "        # ensuring the output from the former matches the input of the latter, in both name and type.\n",
    "        self.add_flow(study_loader_op, series_selector_op, {\"dicom_study_list\": \"dicom_study_list\"})\n",
    "        self.add_flow(series_selector_op, series_to_vol_op, {\"study_selected_series_list\": \"study_selected_series_list\"}        )\n",
    "        self.add_flow(series_to_vol_op, spleen_seg_op, {\"image\": \"image\"})\n",
    "\n",
    "        # Note below the dicom_seg_writer requires two inputs, each coming from a source operator.\n",
    "        self.add_flow(series_selector_op, dicom_seg_writer, {\"study_selected_series_list\": \"study_selected_series_list\"}        )\n",
    "        self.add_flow(spleen_seg_op, dicom_seg_writer, {\"seg_image\": \"seg_image\"})\n",
    "\n",
    "        viz_op = ClaraVizOperator()\n",
    "        self.add_flow(series_to_vol_op, viz_op, {\"image\": \"image\"})\n",
    "        self.add_flow(spleen_seg_op, viz_op, {\"seg_image\": \"seg_image\"})\n",
    "        \n",
    "        self._logger.debug(f\"End {self.compose.__name__}\")\n",
    "\n",
    "\n",
    "# This is a sample series selection rule in JSON, simply selecting CT series.\n",
    "# If the study has more than 1 CT series, then all of them will be selected.\n",
    "# Please see more detail in DICOMSeriesSelectorOperator.\n",
    "# For list of string values, e.g. \"ImageType\": [\"PRIMARY\", \"ORIGINAL\"], it is a match if all elements\n",
    "# are all in the multi-value attribute of the DICOM series.\n",
    "Sample_Rules_Text = \"\"\"\n",
    "{\n",
    "    \"selections\": [\n",
    "        {\n",
    "            \"name\": \"CT Series\",\n",
    "            \"conditions\": {\n",
    "                \"StudyDescription\": \"(.*?)\",\n",
    "                \"Modality\": \"(?i)CT\",\n",
    "                \"SeriesDescription\": \"(.*?)\",\n",
    "                \"ImageType\": [\"PRIMARY\", \"ORIGINAL\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Executing app locally\n",
    "\n",
    "We can execute the app in the Jupyter notebook. Note that the DICOM files of the CT Abdomen series must be present in the `dcm` and the Torch Script model at `model.ts`. Please use the actual path in your environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize segmentation\n",
    "\n",
    "[NVIDIA Clara Viz](https://github.com/NVIDIA/clara-viz) is a platform for visualizing 2D/3D medical imaging data. It enables building applications that leverage powerful volumetric visualization using CUDA-based ray tracing.\n",
    "\n",
    "Clara Viz offers a Python Wrapper for rapid experimentation. It also includes a collection of visual widgets for performing interactive medical image visualization in Jupyter Lab notebooks which we'll use below.  \n",
    "\n",
    "---\n",
    " > **Note**: We'll make sure to remove this when we package the application as a Python application below.\n",
    "---\n",
    "\n",
    "#### Clara Viz Controls\n",
    "\n",
    "After running the cell below, you should see the visualization of the image and segmentation. \n",
    "- You can use the scroll wheel to zoom in/out\n",
    "- Left-click hold then up down to go through the slices \n",
    "- Middle-click hold to pan\n",
    "\n",
    "You can select a view from the drop-down menu to view:\n",
    "- Render view\n",
    "- Image only \n",
    "- Image and segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = AISpleenSegApp()\n",
    "\n",
    "app.run(input=NOTEBOOK_ROOT+\"/dcm\", output=NOTEBOOK_ROOT+\"/output\", model=NOTEBOOK_ROOT+\"/model.ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Files\n",
    "\n",
    "### 3.1 Application Structure\n",
    "\n",
    "Once the application is verified inside Jupyter notebook, we can write the above Python code into Python files in an application folder.\n",
    "\n",
    "The application folder structure would look like below:\n",
    "\n",
    "```bash\n",
    "my_app\n",
    "├── __main__.py\n",
    "├── app.py\n",
    "└── spleen_seg_operator.py\n",
    "```\n",
    "\n",
    "---\n",
    " > **Note**: We can create a single application Python file (such as `spleen_app.py`) that includes the files' content instead of creating multiple files.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an application folder\n",
    "!mkdir -p \"{NOTEBOOK_ROOT}/my_app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 spleen_seg_operator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {NOTEBOOK_ROOT}/my_app/spleen_seg_operator.py\n",
    "import logging\n",
    "from os import path\n",
    "\n",
    "from numpy import uint8\n",
    "\n",
    "import monai.deploy.core as md\n",
    "from monai.deploy.core import ExecutionContext, Image, InputContext, IOType, Operator, OutputContext\n",
    "from monai.deploy.operators.monai_seg_inference_operator import InMemImageReader, MonaiSegInferenceOperator\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    ")\n",
    "\n",
    "\n",
    "@md.input(\"image\", Image, IOType.IN_MEMORY)\n",
    "@md.output(\"seg_image\", Image, IOType.IN_MEMORY)\n",
    "@md.env(pip_packages=[\"monai>=0.8.1\", \"torch>=1.10.2\", \"numpy>=1.21\", \"nibabel\"])\n",
    "class SpleenSegOperator(Operator):\n",
    "    \"\"\"Performs Spleen segmentation with a 3D image converted from a DICOM CT series.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.logger = logging.getLogger(\"{}.{}\".format(__name__, type(self).__name__))\n",
    "        super().__init__()\n",
    "        self._input_dataset_key = \"image\"\n",
    "        self._pred_dataset_key = \"pred\"\n",
    "\n",
    "    def compute(self, op_input: InputContext, op_output: OutputContext, context: ExecutionContext):\n",
    "\n",
    "        input_image = op_input.get(\"image\")\n",
    "        if not input_image:\n",
    "            raise ValueError(\"Input image is not found.\")\n",
    "\n",
    "        output_path = context.output.get().path\n",
    "\n",
    "        # This operator gets an in-memory Image object, so a specialized ImageReader is needed.\n",
    "        _reader = InMemImageReader(input_image)\n",
    "        pre_transforms = self.pre_process(_reader)\n",
    "        post_transforms = self.post_process(pre_transforms, path.join(output_path, \"prediction_output\"))\n",
    "\n",
    "        # Delegates inference and saving output to the built-in operator.\n",
    "        infer_operator = MonaiSegInferenceOperator(\n",
    "            (\n",
    "                96,\n",
    "                96,\n",
    "                96,\n",
    "            ),\n",
    "            pre_transforms,\n",
    "            post_transforms,\n",
    "        )\n",
    "\n",
    "        # Setting the keys used in the dictironary based transforms may change.\n",
    "        infer_operator.input_dataset_key = self._input_dataset_key\n",
    "        infer_operator.pred_dataset_key = self._pred_dataset_key\n",
    "\n",
    "        # Now let the built-in operator handles the work with the I/O spec and execution context.\n",
    "        infer_operator.compute(op_input, op_output, context)\n",
    "\n",
    "    def pre_process(self, img_reader) -> Compose:\n",
    "        \"\"\"Composes transforms for preprocessing input before predicting on a model.\"\"\"\n",
    "\n",
    "        my_key = self._input_dataset_key\n",
    "        return Compose(\n",
    "            [\n",
    "                LoadImaged(keys=my_key, reader=img_reader),\n",
    "                EnsureChannelFirstd(keys=my_key),\n",
    "                Orientationd(keys=my_key, axcodes=\"RAS\"),\n",
    "                Spacingd(keys=my_key, pixdim=[1.5, 1.5, 2.9], mode=[\"bilinear\"]),\n",
    "                ScaleIntensityRanged(keys=my_key, a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "                EnsureTyped(keys=my_key),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def post_process(self, pre_transforms: Compose, out_dir: str = \"./prediction_output\") -> Compose:\n",
    "        \"\"\"Composes transforms for postprocessing the prediction results.\"\"\"\n",
    "\n",
    "        pred_key = self._pred_dataset_key\n",
    "        return Compose(\n",
    "            [\n",
    "                Activationsd(keys=pred_key, softmax=True),\n",
    "                Invertd(\n",
    "                    keys=pred_key,\n",
    "                    transform=pre_transforms,\n",
    "                    orig_keys=self._input_dataset_key,\n",
    "                    nearest_interp=False,\n",
    "                    to_tensor=True,\n",
    "                ),\n",
    "                AsDiscreted(keys=pred_key, argmax=True),\n",
    "                SaveImaged(\n",
    "                    keys=pred_key,\n",
    "                    output_dir=out_dir,\n",
    "                    output_postfix=\"seg\",\n",
    "                    output_dtype=uint8,\n",
    "                ),\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {NOTEBOOK_ROOT}/my_app/app.py\n",
    "import logging\n",
    "\n",
    "from spleen_seg_operator import SpleenSegOperator\n",
    "\n",
    "# Required for setting SegmentDescription attributes. Direct import as this is not part of App SDK package.\n",
    "from pydicom.sr.codedict import codes\n",
    "\n",
    "from monai.deploy.core import Application, resource\n",
    "import monai.deploy.core as md\n",
    "from monai.deploy.operators.dicom_data_loader_operator import DICOMDataLoaderOperator\n",
    "from monai.deploy.operators.dicom_seg_writer_operator import DICOMSegmentationWriterOperator, SegmentDescription\n",
    "from monai.deploy.operators.dicom_series_selector_operator import DICOMSeriesSelectorOperator\n",
    "from monai.deploy.operators.dicom_series_to_volume_operator import DICOMSeriesToVolumeOperator\n",
    "\n",
    "# This is a sample series selection rule in JSON, simply selecting CT series.\n",
    "# If the study has more than 1 CT series, then all of them will be selected.\n",
    "# Please see more detail in DICOMSeriesSelectorOperator.\n",
    "Sample_Rules_Text = \"\"\"\n",
    "{\n",
    "    \"selections\": [\n",
    "        {\n",
    "            \"name\": \"CT Series\",\n",
    "            \"conditions\": {\n",
    "                \"StudyDescription\": \"(.*?)\",\n",
    "                \"Modality\": \"(?i)CT\",\n",
    "                \"SeriesDescription\": \"(.*?)\",\n",
    "                \"ImageType\": [\"PRIMARY\", \"ORIGINAL\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "@resource(cpu=1, gpu=1, memory=\"7Gi\")\n",
    "@md.env(pip_packages=[\"typeguard==2.13.3\"])\n",
    "class AISpleenSegApp(Application):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Creates an application instance.\"\"\"\n",
    "\n",
    "        self._logger = logging.getLogger(\"{}.{}\".format(__name__, type(self).__name__))\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def run(self, *args, **kwargs):\n",
    "        # This method calls the base class to run. Can be omitted if simply calling through.\n",
    "        self._logger.debug(f\"Begin {self.run.__name__}\")\n",
    "        super().run(*args, **kwargs)\n",
    "        self._logger.debug(f\"End {self.run.__name__}\")\n",
    "\n",
    "    def compose(self):\n",
    "        \"\"\"Creates the app specific operators and chain them up in the processing DAG.\"\"\"\n",
    "\n",
    "        self._logger.debug(f\"Begin {self.compose.__name__}\")\n",
    "        # Creates the custom operator(s) as well as SDK built-in operator(s).\n",
    "        study_loader_op = DICOMDataLoaderOperator()\n",
    "        series_selector_op = DICOMSeriesSelectorOperator(rules=Sample_Rules_Text)\n",
    "        series_to_vol_op = DICOMSeriesToVolumeOperator()\n",
    "        # Model specific inference operator, supporting MONAI transforms.\n",
    "\n",
    "        # Creates the model specific segmentation operator\n",
    "        spleen_seg_op = SpleenSegOperator()\n",
    "\n",
    "        # Create DICOM Seg writer providing the required segment description for each segment with\n",
    "        # the actual algorithm and the pertinent organ/tissue.\n",
    "        # The segment_label, algorithm_name, and algorithm_version are limited to 64 chars.\n",
    "        # https://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_6.2.html\n",
    "        # User can Look up SNOMED CT codes at, e.g.\n",
    "        # https://bioportal.bioontology.org/ontologies/SNOMEDCT\n",
    "\n",
    "        _algorithm_name = \"3D segmentation of the Spleen from a CT series\"\n",
    "        _algorithm_family = codes.DCM.ArtificialIntelligence\n",
    "        _algorithm_version = \"0.1.0\"\n",
    "\n",
    "        segment_descriptions = [\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Lung\",\n",
    "                segmented_property_category=codes.SCT.Organ,\n",
    "                segmented_property_type=codes.SCT.Lung,\n",
    "                algorithm_name=_algorithm_name,\n",
    "                algorithm_family=_algorithm_family,\n",
    "                algorithm_version=_algorithm_version,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        custom_tags = {\"SeriesDescription\": \"AI generated Seg, not for clinical use.\"}\n",
    "\n",
    "        dicom_seg_writer = DICOMSegmentationWriterOperator(\n",
    "            segment_descriptions=segment_descriptions, custom_tags=custom_tags\n",
    "        )\n",
    "\n",
    "        # Create the processing pipeline, by specifying the source and destination operators, and\n",
    "        # ensuring the output from the former matches the input of the latter, in both name and type.\n",
    "        self.add_flow(study_loader_op, series_selector_op, {\"dicom_study_list\": \"dicom_study_list\"})\n",
    "        self.add_flow(\n",
    "            series_selector_op, series_to_vol_op, {\"study_selected_series_list\": \"study_selected_series_list\"}\n",
    "        )\n",
    "        self.add_flow(series_to_vol_op, spleen_seg_op, {\"image\": \"image\"})\n",
    "\n",
    "        # Note below the dicom_seg_writer requires two inputs, each coming from a source operator.\n",
    "        self.add_flow(\n",
    "            series_selector_op, dicom_seg_writer, {\"study_selected_series_list\": \"study_selected_series_list\"}\n",
    "        )\n",
    "        self.add_flow(spleen_seg_op, dicom_seg_writer, {\"seg_image\": \"seg_image\"})\n",
    "\n",
    "        self._logger.debug(f\"End {self.compose.__name__}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Creates the app and test it standalone. When running is this mode, please note the following:\n",
    "    #     -i <DICOM folder>, for input DICOM CT series folder\n",
    "    #     -o <output folder>, for the output folder, default $PWD/output\n",
    "    #     -m <model file>, for model file path\n",
    "    # e.g.\n",
    "    #     python3 app.py -i input -m model.ts\n",
    "    #\n",
    "    AISpleenSegApp(do_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 \\_\\_main\\_\\_.py\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    AISpleenSegApp(do_run=True)\n",
    "```\n",
    "\n",
    "The above lines are needed to execute the application code by using `python` interpreter.\n",
    "\n",
    "\\_\\_main\\_\\_.py is needed for <a href=\"../../developing_with_sdk/packaging_app.html#required-arguments\">MONAI Application Packager</a> to detect the main application code (`app.py`) when the application is executed with the application folder path (e.g., `python my_app`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {NOTEBOOK_ROOT}/my_app/__main__.py\n",
    "from app import AISpleenSegApp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    AISpleenSegApp(do_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {NOTEBOOK_ROOT}my_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run_cli'></a>\n",
    "## 3.5 Run App on CLI\n",
    "You can run the App in one of two ways:\n",
    "- Using the python command\n",
    "- Using the 'monai-deploy' command \n",
    "\n",
    "---\n",
    " > **Note**: We are executing python code which makes developing and debugging simple.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python \"{NOTEBOOK_ROOT}/my_app\" -i \"{NOTEBOOK_ROOT}/dcm\" -o \"{NOTEBOOK_ROOT}/output\" -m \"{NOTEBOOK_ROOT}/model.ts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command and below command both run the application, but one uses python directly, and the other uses the provided MONAI Deploy CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "!monai-deploy exec \"{NOTEBOOK_ROOT}/my_app\" -i \"{NOTEBOOK_ROOT}/dcm\" -o \"{NOTEBOOK_ROOT}/output\" -m \"{NOTEBOOK_ROOT}/model.ts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then use the `ls -la` comand to list the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la \"{NOTEBOOK_ROOT}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Packaging app (creating MAP Docker image)\n",
    "\n",
    "### 4.1 Docker intro\n",
    "Why Docker? It's lightweight and reproducible. While VMs encapsulate the entire OS and any applications, containers encapsulate individual applications and their dependencies for portable deployment, but share the same host OS between containers.\n",
    "\n",
    "![Docker Intro](images/docker.png)\n",
    "\n",
    "Examples:\n",
    "- NVIDIA releases docker for each framework every month\n",
    "\n",
    "\n",
    "### 4.2 MONAI Application Package (MAPs)\n",
    "\n",
    "Clara-Viz operators added in an application are used for interactive visualization, so the application shall not be packaged with [MONAI Application Packager](/developing_with_sdk/packaging_app).\n",
    "\n",
    "Let's package the app with [MONAI Application Packager](https://docs.monai.io/projects/monai-deploy-app-sdk/en/latest/developing_with_sdk/packaging_app.html). The MONAI Application Packager (Packager) is a utility for building an application developed with the MONAI Deploy App SDK into a structured MONAI Application Package (MAP).The MAP produced by the Packager is a deployable and reusable docker image that can be launched locally or remotely.\n",
    "\n",
    "Below we'll use the monai-deploy CLI to package the application by using the package command.  We'll pass the `my_app` folder path and use the `my_app` tag. \n",
    "\n",
    "\n",
    "---\n",
    " > **Note**: Building a MONAI Application Package (Docker image) can take time. Use `-l DEBUG` option if you want to see the progress. Also, this won't work in Google Colab.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!monai-deploy package \"{NOTEBOOK_ROOT}/my_app/\" --tag my_app:latest -m \"{NOTEBOOK_ROOT}/model.ts\" -l DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the docker image, we can verify it by using the `docker image ls` command and grepping for `my_app`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker image ls | grep my_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Executing packaged app locally\n",
    "\n",
    "The packaged app can be run locally through [MONAI Application Runner](/developing_with_sdk/executing_packaged_app_locally).\n",
    "\n",
    "\n",
    "---\n",
    " > **Note**: Following command will run on the host machine using docker. \n",
    "Therefore: \n",
    "- Mapping volumes should be the real host path, that is why we use the docker variable LAB_PATH\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the app\n",
    "!monai-deploy run my_app:latest \"{NOTEBOOK_ROOT}/dcm\" \"{NOTEBOOK_ROOT}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have walked through creating a segmentation task and utilizing the existing DICOM Operators provided by MONAI Deploy App SDK. You've run the application in Jupyter, locally using Python and the MONAI Deploy CLI, and finally, you packaged the application using docker and executed the newly created container image.\n",
    "\n",
    "You also worked on some additional exercises to help you get familiar with working in the MONAI Deploy Workflow.\n",
    "\n",
    "### What's Next\n",
    "You're now ready to use MONAI Deploy App SDK and integrate a custom model.  We'll give you an example to work on in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b4ab1155d0cd1042497eb40fd55b2d15caf4b3c0f9fbfcc7ba4404045d40f12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
